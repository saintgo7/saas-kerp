# 25. 재해복구 계획 (Disaster Recovery Plan)

## 목차
1. [개요](#1-개요)
2. [복구 목표](#2-복구-목표)
3. [재해 시나리오](#3-재해-시나리오)
4. [백업 전략](#4-백업-전략)
5. [복구 절차](#5-복구-절차)
6. [멀티 리전 아키텍처](#6-멀티-리전-아키텍처)
7. [테스트 및 훈련](#7-테스트-및-훈련)
8. [커뮤니케이션 계획](#8-커뮤니케이션-계획)

---

## 1. 개요

### 1.1 목적

```yaml
disaster_recovery_goals:
  business_continuity:
    description: "비즈니스 연속성 보장"
    target: "핵심 서비스 4시간 내 복구"

  data_protection:
    description: "데이터 손실 최소화"
    target: "최대 1시간 데이터 손실 허용"

  compliance:
    description: "규정 준수"
    requirements:
      - "전자금융거래법"
      - "개인정보보호법"
      - "K-IFRS 데이터 보존"

  resilience:
    description: "시스템 복원력"
    target: "연간 가동률 99.9%"
```

### 1.2 시스템 구성 개요

```
┌──────────────────────────────────────────────────────────────────┐
│                    K-ERP Multi-Region Architecture                │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Primary Region (ap-northeast-2)     DR Region (ap-northeast-1)  │
│  ┌─────────────────────────────┐    ┌─────────────────────────┐  │
│  │        EKS Cluster          │    │     EKS Cluster (Warm)  │  │
│  │  ┌─────────────────────┐   │    │  ┌─────────────────────┐ │  │
│  │  │    Go API Pods      │   │    │  │   Go API Pods (0)   │ │  │
│  │  │    (3 replicas)     │   │    │  │   (scale on DR)     │ │  │
│  │  └─────────────────────┘   │    │  └─────────────────────┘ │  │
│  │  ┌─────────────────────┐   │    │  ┌─────────────────────┐ │  │
│  │  │   Python gRPC Pods  │   │    │  │  Python gRPC (0)    │ │  │
│  │  │    (2 replicas)     │   │    │  │  (scale on DR)      │ │  │
│  │  └─────────────────────┘   │    │  └─────────────────────┘ │  │
│  └─────────────────────────────┘    └─────────────────────────┘  │
│                                                                   │
│  ┌─────────────────────────────┐    ┌─────────────────────────┐  │
│  │    RDS PostgreSQL          │────→│  RDS Read Replica       │  │
│  │    (Primary)               │sync │  (Cross-Region)         │  │
│  └─────────────────────────────┘    └─────────────────────────┘  │
│                                                                   │
│  ┌─────────────────────────────┐    ┌─────────────────────────┐  │
│  │    ElastiCache Redis       │────→│  ElastiCache Redis      │  │
│  │    (Primary)               │repl │  (Global Datastore)     │  │
│  └─────────────────────────────┘    └─────────────────────────┘  │
│                                                                   │
│  ┌─────────────────────────────┐    ┌─────────────────────────┐  │
│  │    S3 Bucket               │────→│  S3 Bucket              │  │
│  │    (Primary)               │CRR  │  (Cross-Region Repl)    │  │
│  └─────────────────────────────┘    └─────────────────────────┘  │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

---

## 2. 복구 목표

### 2.1 RTO/RPO 정의

```yaml
recovery_objectives:
  critical_services:
    description: "핵심 비즈니스 서비스"
    services:
      - "인증/인가"
      - "전표 조회"
      - "세금계산서 발행"
    rto: "4시간"
    rpo: "1시간"
    priority: "P1"

  important_services:
    description: "주요 업무 서비스"
    services:
      - "전표 생성/수정"
      - "원장 조회"
      - "보고서 생성"
    rto: "8시간"
    rpo: "4시간"
    priority: "P2"

  standard_services:
    description: "일반 서비스"
    services:
      - "기준정보 관리"
      - "사용자 설정"
      - "알림 발송"
    rto: "24시간"
    rpo: "24시간"
    priority: "P3"

definitions:
  rto: "Recovery Time Objective - 복구 목표 시간"
  rpo: "Recovery Point Objective - 복구 시점 목표 (허용 데이터 손실)"
```

### 2.2 서비스 등급별 SLA

```yaml
service_tiers:
  tier1_platinum:
    description: "Enterprise Plus 고객"
    availability: "99.99%"
    rto: "1시간"
    rpo: "15분"
    support: "24/7 전담"
    dr_region: "active-active"

  tier2_gold:
    description: "Enterprise 고객"
    availability: "99.9%"
    rto: "4시간"
    rpo: "1시간"
    support: "24/7"
    dr_region: "warm-standby"

  tier3_silver:
    description: "Standard 고객"
    availability: "99.5%"
    rto: "8시간"
    rpo: "4시간"
    support: "업무시간"
    dr_region: "pilot-light"
```

### 2.3 복구 우선순위 매트릭스

```go
// pkg/disaster/priority.go
package disaster

type ServicePriority int

const (
    PriorityCritical ServicePriority = 1
    PriorityHigh     ServicePriority = 2
    PriorityMedium   ServicePriority = 3
    PriorityLow      ServicePriority = 4
)

type Service struct {
    Name           string
    Priority       ServicePriority
    Dependencies   []string
    RTOMinutes     int
    RPOMinutes     int
    RecoverySteps  []string
}

var ServiceCatalog = []Service{
    {
        Name:       "authentication",
        Priority:   PriorityCritical,
        Dependencies: []string{"postgresql", "redis"},
        RTOMinutes: 60,
        RPOMinutes: 15,
        RecoverySteps: []string{
            "Verify database connectivity",
            "Verify Redis connectivity",
            "Deploy auth service pods",
            "Verify JWT signing keys",
            "Run health checks",
        },
    },
    {
        Name:       "api-gateway",
        Priority:   PriorityCritical,
        Dependencies: []string{"authentication"},
        RTOMinutes: 60,
        RPOMinutes: 0,
        RecoverySteps: []string{
            "Deploy API gateway pods",
            "Configure SSL certificates",
            "Update DNS records",
            "Verify routing rules",
        },
    },
    {
        Name:       "voucher-service",
        Priority:   PriorityHigh,
        Dependencies: []string{"postgresql", "api-gateway"},
        RTOMinutes: 120,
        RPOMinutes: 60,
        RecoverySteps: []string{
            "Verify database replication",
            "Deploy voucher service pods",
            "Verify data integrity",
            "Run reconciliation checks",
        },
    },
    {
        Name:       "tax-invoice-service",
        Priority:   PriorityHigh,
        Dependencies: []string{"voucher-service", "python-grpc"},
        RTOMinutes: 180,
        RPOMinutes: 60,
        RecoverySteps: []string{
            "Deploy Go service pods",
            "Deploy Python gRPC pods",
            "Verify Popbill connectivity",
            "Test invoice generation",
        },
    },
}
```

---

## 3. 재해 시나리오

### 3.1 시나리오 분류

```yaml
disaster_scenarios:
  level1_minor:
    description: "경미한 장애"
    examples:
      - "단일 Pod 장애"
      - "단일 Node 장애"
      - "네트워크 일시 불안정"
    response: "자동 복구 (K8s self-healing)"
    expected_downtime: "< 5분"

  level2_moderate:
    description: "중간 수준 장애"
    examples:
      - "AZ 장애"
      - "데이터베이스 failover"
      - "외부 API 장애 (Popbill 등)"
    response: "반자동 복구 + 수동 확인"
    expected_downtime: "< 30분"

  level3_major:
    description: "주요 장애"
    examples:
      - "리전 장애"
      - "데이터 손상"
      - "보안 침해"
    response: "DR 사이트 활성화"
    expected_downtime: "< 4시간"

  level4_catastrophic:
    description: "대규모 재해"
    examples:
      - "자연재해 (지진, 홍수)"
      - "장기 전력 장애"
      - "랜섬웨어 공격"
    response: "완전 DR 전환"
    expected_downtime: "< 24시간"
```

### 3.2 장애 감지 시스템

```go
// pkg/disaster/detector.go
package disaster

import (
    "context"
    "time"

    "github.com/prometheus/client_golang/prometheus"
)

type DisasterLevel int

const (
    LevelNormal DisasterLevel = iota
    LevelMinor
    LevelModerate
    LevelMajor
    LevelCatastrophic
)

type HealthIndicator struct {
    Name      string
    Status    string
    LastCheck time.Time
    Details   map[string]interface{}
}

type DisasterDetector struct {
    indicators       map[string]*HealthIndicator
    alertManager     AlertManager
    metricsCollector *prometheus.GaugeVec
}

func NewDisasterDetector(am AlertManager) *DisasterDetector {
    return &DisasterDetector{
        indicators:   make(map[string]*HealthIndicator),
        alertManager: am,
        metricsCollector: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "kerp_disaster_level",
                Help: "Current disaster level",
            },
            []string{"region"},
        ),
    }
}

func (d *DisasterDetector) EvaluateDisasterLevel(ctx context.Context) DisasterLevel {
    checks := []struct {
        name    string
        checker func(context.Context) bool
        weight  int
    }{
        {"database_primary", d.checkDatabasePrimary, 10},
        {"database_replica", d.checkDatabaseReplica, 5},
        {"redis_cluster", d.checkRedisCluster, 5},
        {"kubernetes_nodes", d.checkKubernetesNodes, 8},
        {"api_health", d.checkAPIHealth, 7},
        {"external_apis", d.checkExternalAPIs, 3},
    }

    failedWeight := 0
    for _, check := range checks {
        if !check.checker(ctx) {
            failedWeight += check.weight
            d.indicators[check.name] = &HealthIndicator{
                Name:      check.name,
                Status:    "unhealthy",
                LastCheck: time.Now(),
            }
        }
    }

    switch {
    case failedWeight >= 30:
        return LevelCatastrophic
    case failedWeight >= 20:
        return LevelMajor
    case failedWeight >= 10:
        return LevelModerate
    case failedWeight >= 5:
        return LevelMinor
    default:
        return LevelNormal
    }
}

func (d *DisasterDetector) checkDatabasePrimary(ctx context.Context) bool {
    // Check primary database connectivity and replication lag
    return true
}

func (d *DisasterDetector) checkDatabaseReplica(ctx context.Context) bool {
    // Check cross-region replica status
    return true
}

func (d *DisasterDetector) checkRedisCluster(ctx context.Context) bool {
    // Check Redis cluster health
    return true
}

func (d *DisasterDetector) checkKubernetesNodes(ctx context.Context) bool {
    // Check K8s node status
    return true
}

func (d *DisasterDetector) checkAPIHealth(ctx context.Context) bool {
    // Check API endpoint health
    return true
}

func (d *DisasterDetector) checkExternalAPIs(ctx context.Context) bool {
    // Check external API connectivity
    return true
}
```

### 3.3 자동 페일오버 조건

```yaml
# deployments/k8s/disaster/failover-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: failover-config
  namespace: kerp-system
data:
  config.yaml: |
    failover:
      automatic:
        enabled: true
        conditions:
          database:
            - type: "replication_lag"
              threshold: "300s"
              action: "alert"
            - type: "replication_lag"
              threshold: "600s"
              action: "promote_replica"
            - type: "connection_failure"
              duration: "60s"
              action: "promote_replica"

          kubernetes:
            - type: "node_not_ready"
              count: 2
              duration: "300s"
              action: "drain_and_replace"
            - type: "api_server_unreachable"
              duration: "60s"
              action: "activate_dr"

          application:
            - type: "error_rate"
              threshold: "50%"
              duration: "300s"
              action: "rollback"
            - type: "latency_p99"
              threshold: "5000ms"
              duration: "300s"
              action: "scale_up"

        cooldown:
          database_failover: "1h"
          dr_activation: "24h"

      manual_approval_required:
        - "dr_region_switch"
        - "data_restore_from_backup"
        - "database_point_in_time_recovery"
```

---

## 4. 백업 전략

### 4.1 백업 계층

```yaml
backup_layers:
  layer1_realtime:
    type: "실시간 복제"
    components:
      postgresql:
        method: "Streaming Replication"
        target: "Cross-Region Read Replica"
        lag_target: "< 1분"
      redis:
        method: "Global Datastore"
        target: "Cross-Region Replica"
      s3:
        method: "Cross-Region Replication"
        target: "DR Region Bucket"

  layer2_snapshot:
    type: "주기적 스냅샷"
    components:
      postgresql:
        frequency: "1시간"
        retention: "7일"
        storage: "S3 (encrypted)"
      redis:
        frequency: "6시간"
        retention: "3일"
      ebs:
        frequency: "일일"
        retention: "30일"

  layer3_archive:
    type: "장기 보관"
    components:
      database:
        frequency: "일일"
        retention: "10년"
        storage: "S3 Glacier"
        compliance: "K-IFRS"
      logs:
        retention: "5년"
        storage: "S3 Glacier"
      audit:
        retention: "10년"
        storage: "S3 Glacier Deep Archive"
```

### 4.2 PostgreSQL 백업

```bash
#!/bin/bash
# scripts/backup/postgresql_backup.sh

set -euo pipefail

# Configuration
DB_HOST="${DB_HOST:-kerp-db.cluster-xxxxx.ap-northeast-2.rds.amazonaws.com}"
DB_NAME="${DB_NAME:-kerp_production}"
BACKUP_BUCKET="${BACKUP_BUCKET:-kerp-backups-prod}"
BACKUP_PREFIX="postgresql/daily"
RETENTION_DAYS=30
ENCRYPTION_KEY_ID="${KMS_KEY_ID}"

# Timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="kerp_${TIMESTAMP}.sql.gz.enc"

echo "Starting PostgreSQL backup at $(date)"

# Create backup with compression and encryption
pg_dump -h "${DB_HOST}" \
    -U "${DB_USER}" \
    -d "${DB_NAME}" \
    --format=custom \
    --compress=9 \
    --no-owner \
    --no-acl \
    --verbose \
    2>&1 | \
    aws kms encrypt \
        --key-id "${ENCRYPTION_KEY_ID}" \
        --plaintext fileb:///dev/stdin \
        --output text \
        --query CiphertextBlob | \
    base64 --decode > "/tmp/${BACKUP_FILE}"

# Upload to S3 with server-side encryption
aws s3 cp "/tmp/${BACKUP_FILE}" \
    "s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/${BACKUP_FILE}" \
    --sse aws:kms \
    --sse-kms-key-id "${ENCRYPTION_KEY_ID}" \
    --storage-class STANDARD_IA

# Verify upload
UPLOAD_SIZE=$(aws s3api head-object \
    --bucket "${BACKUP_BUCKET}" \
    --key "${BACKUP_PREFIX}/${BACKUP_FILE}" \
    --query 'ContentLength' \
    --output text)

if [ "${UPLOAD_SIZE}" -gt 0 ]; then
    echo "Backup uploaded successfully: ${BACKUP_FILE} (${UPLOAD_SIZE} bytes)"
else
    echo "ERROR: Backup upload failed"
    exit 1
fi

# Cleanup old backups
echo "Cleaning up backups older than ${RETENTION_DAYS} days"
aws s3 ls "s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/" | \
    while read -r line; do
        createDate=$(echo "$line" | awk '{print $1" "$2}')
        createDate=$(date -d "${createDate}" +%s)
        olderThan=$(date -d "-${RETENTION_DAYS} days" +%s)
        if [[ ${createDate} -lt ${olderThan} ]]; then
            fileName=$(echo "$line" | awk '{print $4}')
            if [[ -n "${fileName}" ]]; then
                aws s3 rm "s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/${fileName}"
                echo "Deleted old backup: ${fileName}"
            fi
        fi
    done

# Record backup metadata
aws dynamodb put-item \
    --table-name kerp-backup-metadata \
    --item '{
        "backup_id": {"S": "'"${TIMESTAMP}"'"},
        "backup_type": {"S": "postgresql_full"},
        "file_path": {"S": "s3://'"${BACKUP_BUCKET}/${BACKUP_PREFIX}/${BACKUP_FILE}"'"},
        "size_bytes": {"N": "'"${UPLOAD_SIZE}"'"},
        "created_at": {"S": "'"$(date -Iseconds)"'"},
        "status": {"S": "completed"}
    }'

echo "Backup completed at $(date)"

# Clean up local file
rm -f "/tmp/${BACKUP_FILE}"
```

### 4.3 Redis 백업

```yaml
# deployments/k8s/backup/redis-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: kerp-system
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-service
          containers:
          - name: redis-backup
            image: kerp/backup-tools:latest
            command:
            - /bin/sh
            - -c
            - |
              set -e

              # Trigger Redis BGSAVE
              redis-cli -h ${REDIS_HOST} -a ${REDIS_PASSWORD} BGSAVE

              # Wait for BGSAVE to complete
              while [ $(redis-cli -h ${REDIS_HOST} -a ${REDIS_PASSWORD} LASTSAVE) == $(redis-cli -h ${REDIS_HOST} -a ${REDIS_PASSWORD} LASTSAVE) ]; do
                sleep 1
              done

              # Copy RDB file to S3
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              aws s3 cp /data/dump.rdb s3://${BACKUP_BUCKET}/redis/${TIMESTAMP}_dump.rdb \
                --sse aws:kms \
                --sse-kms-key-id ${KMS_KEY_ID}

              echo "Redis backup completed: ${TIMESTAMP}"
            env:
            - name: REDIS_HOST
              valueFrom:
                secretKeyRef:
                  name: redis-credentials
                  key: host
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: redis-credentials
                  key: password
            - name: BACKUP_BUCKET
              value: "kerp-backups-prod"
            - name: KMS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-kms
                  key: key-id
            volumeMounts:
            - name: redis-data
              mountPath: /data
          volumes:
          - name: redis-data
            persistentVolumeClaim:
              claimName: redis-data-pvc
          restartPolicy: OnFailure
```

### 4.4 애플리케이션 데이터 백업

```go
// internal/backup/manager.go
package backup

import (
    "context"
    "fmt"
    "time"

    "github.com/aws/aws-sdk-go-v2/service/s3"
)

type BackupManager struct {
    s3Client     *s3.Client
    bucket       string
    kmsKeyID     string
    metadataRepo BackupMetadataRepository
}

type BackupJob struct {
    ID          string
    Type        BackupType
    Status      BackupStatus
    StartedAt   time.Time
    CompletedAt *time.Time
    SizeBytes   int64
    FilePath    string
    Checksum    string
    Error       string
}

type BackupType string

const (
    BackupTypeFull        BackupType = "full"
    BackupTypeIncremental BackupType = "incremental"
    BackupTypeTransaction BackupType = "transaction"
)

type BackupStatus string

const (
    BackupStatusPending   BackupStatus = "pending"
    BackupStatusRunning   BackupStatus = "running"
    BackupStatusCompleted BackupStatus = "completed"
    BackupStatusFailed    BackupStatus = "failed"
)

func (m *BackupManager) CreateBackup(ctx context.Context, backupType BackupType) (*BackupJob, error) {
    job := &BackupJob{
        ID:        generateBackupID(),
        Type:      backupType,
        Status:    BackupStatusPending,
        StartedAt: time.Now(),
    }

    // Save job metadata
    if err := m.metadataRepo.Save(ctx, job); err != nil {
        return nil, fmt.Errorf("failed to save backup metadata: %w", err)
    }

    // Start backup in background
    go m.runBackup(context.Background(), job)

    return job, nil
}

func (m *BackupManager) runBackup(ctx context.Context, job *BackupJob) {
    job.Status = BackupStatusRunning
    m.metadataRepo.Save(ctx, job)

    defer func() {
        completedAt := time.Now()
        job.CompletedAt = &completedAt
        m.metadataRepo.Save(ctx, job)
    }()

    switch job.Type {
    case BackupTypeFull:
        if err := m.fullBackup(ctx, job); err != nil {
            job.Status = BackupStatusFailed
            job.Error = err.Error()
            return
        }
    case BackupTypeIncremental:
        if err := m.incrementalBackup(ctx, job); err != nil {
            job.Status = BackupStatusFailed
            job.Error = err.Error()
            return
        }
    case BackupTypeTransaction:
        if err := m.transactionLogBackup(ctx, job); err != nil {
            job.Status = BackupStatusFailed
            job.Error = err.Error()
            return
        }
    }

    job.Status = BackupStatusCompleted
}

func (m *BackupManager) fullBackup(ctx context.Context, job *BackupJob) error {
    // Full database backup with all tenant data
    // Uses pg_dump with encryption
    return nil
}

func (m *BackupManager) incrementalBackup(ctx context.Context, job *BackupJob) error {
    // Incremental backup using WAL archiving
    return nil
}

func (m *BackupManager) transactionLogBackup(ctx context.Context, job *BackupJob) error {
    // Transaction log backup for point-in-time recovery
    return nil
}

// RestoreBackup restores from a specific backup
func (m *BackupManager) RestoreBackup(ctx context.Context, backupID string, targetTime *time.Time) error {
    job, err := m.metadataRepo.FindByID(ctx, backupID)
    if err != nil {
        return fmt.Errorf("backup not found: %w", err)
    }

    // Download backup from S3
    // Decrypt and decompress
    // Restore to database
    // Apply transaction logs up to targetTime if specified

    return nil
}
```

---

## 5. 복구 절차

### 5.1 복구 절차 개요

```yaml
recovery_procedure:
  phase1_assessment:
    name: "상황 평가"
    duration: "15분"
    steps:
      - "장애 범위 파악"
      - "데이터 손실 범위 확인"
      - "복구 전략 결정"
      - "이해관계자 통보"

  phase2_containment:
    name: "피해 억제"
    duration: "30분"
    steps:
      - "영향받는 서비스 격리"
      - "트래픽 리다이렉션"
      - "추가 손상 방지"

  phase3_recovery:
    name: "복구 실행"
    duration: "2-4시간"
    steps:
      - "인프라 복구"
      - "데이터 복원"
      - "서비스 재시작"
      - "데이터 검증"

  phase4_validation:
    name: "검증"
    duration: "1시간"
    steps:
      - "기능 테스트"
      - "데이터 정합성 검증"
      - "성능 확인"
      - "보안 점검"

  phase5_normalization:
    name: "정상화"
    duration: "지속"
    steps:
      - "트래픽 복원"
      - "모니터링 강화"
      - "사후 보고서 작성"
```

### 5.2 데이터베이스 복구

```bash
#!/bin/bash
# scripts/disaster/recover_database.sh

set -euo pipefail

BACKUP_BUCKET="${BACKUP_BUCKET:-kerp-backups-prod}"
DR_DB_HOST="${DR_DB_HOST}"
RESTORE_POINT="${1:-latest}"  # Backup ID or 'latest'
TARGET_TIME="${2:-}"          # Optional point-in-time

echo "===== K-ERP Database Recovery Script ====="
echo "Restore Point: ${RESTORE_POINT}"
echo "Target Time: ${TARGET_TIME:-N/A (latest)}"
echo "=========================================="

# Step 1: Find backup to restore
if [ "${RESTORE_POINT}" = "latest" ]; then
    BACKUP_FILE=$(aws s3 ls "s3://${BACKUP_BUCKET}/postgresql/daily/" | \
        sort | tail -1 | awk '{print $4}')
else
    BACKUP_FILE="${RESTORE_POINT}.sql.gz.enc"
fi

echo "Using backup: ${BACKUP_FILE}"

# Step 2: Download and decrypt backup
echo "Downloading backup..."
aws s3 cp "s3://${BACKUP_BUCKET}/postgresql/daily/${BACKUP_FILE}" /tmp/restore.sql.gz.enc

echo "Decrypting backup..."
aws kms decrypt \
    --ciphertext-blob fileb:///tmp/restore.sql.gz.enc \
    --output text \
    --query Plaintext | \
    base64 --decode > /tmp/restore.sql.gz

gunzip /tmp/restore.sql.gz

# Step 3: Promote read replica to primary (if DR scenario)
if [ -n "${PROMOTE_REPLICA}" ]; then
    echo "Promoting DR replica to primary..."
    aws rds promote-read-replica \
        --db-instance-identifier kerp-db-replica-dr \
        --backup-retention-period 7

    # Wait for promotion
    aws rds wait db-instance-available \
        --db-instance-identifier kerp-db-replica-dr
fi

# Step 4: Restore database
echo "Restoring database..."
psql -h "${DR_DB_HOST}" \
    -U "${DB_USER}" \
    -d postgres \
    -c "DROP DATABASE IF EXISTS kerp_production;"

psql -h "${DR_DB_HOST}" \
    -U "${DB_USER}" \
    -d postgres \
    -c "CREATE DATABASE kerp_production;"

pg_restore -h "${DR_DB_HOST}" \
    -U "${DB_USER}" \
    -d kerp_production \
    --verbose \
    --no-owner \
    --no-acl \
    /tmp/restore.sql

# Step 5: Apply WAL logs for point-in-time recovery
if [ -n "${TARGET_TIME}" ]; then
    echo "Applying WAL logs up to ${TARGET_TIME}..."
    # Point-in-time recovery using WAL archive
    aws s3 sync "s3://${BACKUP_BUCKET}/postgresql/wal/" /tmp/wal/

    # Apply WAL files
    for wal_file in /tmp/wal/*; do
        pg_waldump "${wal_file}" | \
            psql -h "${DR_DB_HOST}" -U "${DB_USER}" -d kerp_production
    done
fi

# Step 6: Verify restoration
echo "Verifying restoration..."
VOUCHER_COUNT=$(psql -h "${DR_DB_HOST}" \
    -U "${DB_USER}" \
    -d kerp_production \
    -t -c "SELECT COUNT(*) FROM vouchers;")

COMPANY_COUNT=$(psql -h "${DR_DB_HOST}" \
    -U "${DB_USER}" \
    -d kerp_production \
    -t -c "SELECT COUNT(*) FROM companies;")

echo "Restoration complete!"
echo "- Vouchers: ${VOUCHER_COUNT}"
echo "- Companies: ${COMPANY_COUNT}"

# Cleanup
rm -f /tmp/restore.sql*
rm -rf /tmp/wal/
```

### 5.3 Kubernetes 클러스터 복구

```yaml
# scripts/disaster/recover_kubernetes.yaml
# Ansible playbook for K8s cluster recovery

---
- name: K-ERP Kubernetes Cluster Recovery
  hosts: dr_control_plane
  become: yes
  vars:
    dr_region: ap-northeast-1
    cluster_name: kerp-eks-dr

  tasks:
    - name: Verify EKS cluster status
      shell: |
        aws eks describe-cluster \
          --region {{ dr_region }} \
          --name {{ cluster_name }} \
          --query 'cluster.status'
      register: cluster_status

    - name: Update kubeconfig for DR cluster
      shell: |
        aws eks update-kubeconfig \
          --region {{ dr_region }} \
          --name {{ cluster_name }}

    - name: Scale up node groups
      shell: |
        aws eks update-nodegroup-config \
          --region {{ dr_region }} \
          --cluster-name {{ cluster_name }} \
          --nodegroup-name kerp-workers \
          --scaling-config minSize=3,maxSize=10,desiredSize=5

    - name: Wait for nodes to be ready
      shell: |
        kubectl wait --for=condition=Ready nodes --all --timeout=600s

    - name: Apply core infrastructure
      shell: |
        kubectl apply -k deployments/k8s/overlays/dr/

    - name: Deploy database connection secrets
      shell: |
        kubectl create secret generic db-credentials \
          --from-literal=host={{ dr_db_host }} \
          --from-literal=password={{ dr_db_password }} \
          --namespace kerp-production \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Deploy application services
      shell: |
        kubectl apply -f deployments/k8s/apps/ --recursive

    - name: Scale Go API deployment
      shell: |
        kubectl scale deployment kerp-api \
          --replicas=3 \
          --namespace kerp-production

    - name: Scale Python gRPC deployment
      shell: |
        kubectl scale deployment tax-scraper \
          --replicas=2 \
          --namespace kerp-production
        kubectl scale deployment insurance-edi \
          --replicas=2 \
          --namespace kerp-production

    - name: Wait for deployments to be ready
      shell: |
        kubectl rollout status deployment/kerp-api -n kerp-production --timeout=300s
        kubectl rollout status deployment/tax-scraper -n kerp-production --timeout=300s

    - name: Run health checks
      shell: |
        kubectl run health-check --rm -i --restart=Never \
          --image=curlimages/curl \
          -- curl -f http://kerp-api.kerp-production.svc.cluster.local/health

    - name: Update DNS to point to DR region
      shell: |
        aws route53 change-resource-record-sets \
          --hosted-zone-id {{ hosted_zone_id }} \
          --change-batch '{
            "Changes": [{
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "api.kerp.io",
                "Type": "A",
                "AliasTarget": {
                  "HostedZoneId": "{{ dr_alb_zone_id }}",
                  "DNSName": "{{ dr_alb_dns }}",
                  "EvaluateTargetHealth": true
                }
              }
            }]
          }'
```

### 5.4 데이터 검증

```go
// internal/disaster/validator.go
package disaster

import (
    "context"
    "fmt"

    "github.com/company/kerp/internal/repository"
)

type DataValidator struct {
    voucherRepo  repository.VoucherRepository
    accountRepo  repository.AccountRepository
    companyRepo  repository.CompanyRepository
}

type ValidationResult struct {
    Category    string
    Check       string
    Status      string // "pass", "fail", "warning"
    Details     string
    RecordCount int64
}

func (v *DataValidator) ValidateRestoredData(ctx context.Context) ([]ValidationResult, error) {
    var results []ValidationResult

    // Check 1: Verify company count
    companyCount, err := v.companyRepo.Count(ctx)
    if err != nil {
        return nil, err
    }
    results = append(results, ValidationResult{
        Category:    "Companies",
        Check:       "Company records exist",
        Status:      statusFromCount(companyCount, 1),
        RecordCount: companyCount,
    })

    // Check 2: Verify voucher integrity
    unbalancedCount, err := v.voucherRepo.CountUnbalanced(ctx)
    if err != nil {
        return nil, err
    }
    results = append(results, ValidationResult{
        Category: "Vouchers",
        Check:    "All vouchers balanced",
        Status:   statusFromCount(0, unbalancedCount),
        Details:  fmt.Sprintf("%d unbalanced vouchers found", unbalancedCount),
    })

    // Check 3: Verify account hierarchy
    orphanAccounts, err := v.accountRepo.FindOrphans(ctx)
    if err != nil {
        return nil, err
    }
    results = append(results, ValidationResult{
        Category: "Accounts",
        Check:    "Account hierarchy intact",
        Status:   statusFromCount(0, int64(len(orphanAccounts))),
        Details:  fmt.Sprintf("%d orphan accounts", len(orphanAccounts)),
    })

    // Check 4: Verify foreign key relationships
    fkViolations, err := v.checkForeignKeyIntegrity(ctx)
    if err != nil {
        return nil, err
    }
    results = append(results, ValidationResult{
        Category: "Integrity",
        Check:    "Foreign key relationships",
        Status:   statusFromCount(0, int64(fkViolations)),
        Details:  fmt.Sprintf("%d FK violations", fkViolations),
    })

    // Check 5: Verify audit trail continuity
    auditGaps, err := v.checkAuditTrailContinuity(ctx)
    if err != nil {
        return nil, err
    }
    results = append(results, ValidationResult{
        Category: "Audit",
        Check:    "Audit trail continuity",
        Status:   statusFromCount(0, int64(auditGaps)),
        Details:  fmt.Sprintf("%d gaps in audit trail", auditGaps),
    })

    return results, nil
}

func (v *DataValidator) checkForeignKeyIntegrity(ctx context.Context) (int, error) {
    // Check all FK relationships
    queries := []string{
        `SELECT COUNT(*) FROM voucher_lines vl
         WHERE NOT EXISTS (SELECT 1 FROM vouchers v WHERE v.id = vl.voucher_id)`,
        `SELECT COUNT(*) FROM vouchers v
         WHERE NOT EXISTS (SELECT 1 FROM companies c WHERE c.id = v.company_id)`,
    }

    total := 0
    for _, q := range queries {
        var count int
        // Execute query
        total += count
    }
    return total, nil
}

func (v *DataValidator) checkAuditTrailContinuity(ctx context.Context) (int, error) {
    // Check for gaps in audit sequence numbers
    return 0, nil
}

func statusFromCount(expected, actual int64) string {
    if actual == expected {
        return "pass"
    }
    return "fail"
}
```

---

## 6. 멀티 리전 아키텍처

### 6.1 리전 구성

```yaml
multi_region_config:
  primary_region:
    name: "ap-northeast-2"  # Seoul
    role: "primary"
    services:
      - "All production workloads"
      - "Primary database"
      - "Primary cache"

  dr_region:
    name: "ap-northeast-1"  # Tokyo
    role: "disaster-recovery"
    mode: "warm-standby"
    services:
      - "Database read replica"
      - "Redis replica"
      - "Minimal compute (scaled down)"

  replication:
    database:
      type: "async"
      lag_target: "< 1 minute"
    redis:
      type: "async"
      lag_target: "< 5 seconds"
    s3:
      type: "cross-region-replication"
      lag_target: "< 15 minutes"
```

### 6.2 Terraform 멀티 리전 구성

```hcl
# terraform/dr/main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Primary region provider
provider "aws" {
  alias  = "primary"
  region = "ap-northeast-2"
}

# DR region provider
provider "aws" {
  alias  = "dr"
  region = "ap-northeast-1"
}

# Cross-region RDS read replica
resource "aws_db_instance" "dr_replica" {
  provider = aws.dr

  identifier     = "kerp-db-dr-replica"
  instance_class = "db.r6g.large"

  replicate_source_db = aws_db_instance.primary.arn

  storage_encrypted = true
  kms_key_id        = aws_kms_key.dr_key.arn

  vpc_security_group_ids = [aws_security_group.dr_db.id]
  db_subnet_group_name   = aws_db_subnet_group.dr.name

  backup_retention_period = 7
  skip_final_snapshot     = false

  tags = {
    Name        = "kerp-db-dr-replica"
    Environment = "dr"
    Purpose     = "disaster-recovery"
  }
}

# ElastiCache Global Datastore
resource "aws_elasticache_global_replication_group" "redis" {
  global_replication_group_id_suffix = "kerp-redis-global"
  primary_replication_group_id       = aws_elasticache_replication_group.primary.id

  # Global datastore configuration
  automatic_failover_enabled = true
}

resource "aws_elasticache_replication_group" "dr" {
  provider = aws.dr

  replication_group_id = "kerp-redis-dr"
  description          = "K-ERP Redis DR"

  global_replication_group_id = aws_elasticache_global_replication_group.redis.global_replication_group_id

  node_type            = "cache.r6g.large"
  num_cache_clusters   = 2
  automatic_failover_enabled = true

  subnet_group_name  = aws_elasticache_subnet_group.dr.name
  security_group_ids = [aws_security_group.dr_redis.id]
}

# S3 Cross-Region Replication
resource "aws_s3_bucket" "dr_bucket" {
  provider = aws.dr

  bucket = "kerp-data-dr"

  tags = {
    Name        = "kerp-data-dr"
    Environment = "dr"
  }
}

resource "aws_s3_bucket_versioning" "dr_bucket" {
  provider = aws.dr
  bucket   = aws_s3_bucket.dr_bucket.id

  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_replication_configuration" "primary_to_dr" {
  provider = aws.primary

  bucket = aws_s3_bucket.primary.id
  role   = aws_iam_role.replication.arn

  rule {
    id     = "replicate-all"
    status = "Enabled"

    destination {
      bucket        = aws_s3_bucket.dr_bucket.arn
      storage_class = "STANDARD_IA"

      encryption_configuration {
        replica_kms_key_id = aws_kms_key.dr_key.arn
      }
    }

    source_selection_criteria {
      sse_kms_encrypted_objects {
        status = "Enabled"
      }
    }
  }
}

# DR EKS Cluster (warm standby)
module "eks_dr" {
  source = "../modules/eks"
  providers = {
    aws = aws.dr
  }

  cluster_name    = "kerp-eks-dr"
  cluster_version = "1.28"

  vpc_id     = module.vpc_dr.vpc_id
  subnet_ids = module.vpc_dr.private_subnets

  # Minimal node group for warm standby
  node_groups = {
    workers = {
      desired_capacity = 0  # Scaled to 0 normally
      min_capacity     = 0
      max_capacity     = 10
      instance_types   = ["m6i.xlarge"]

      labels = {
        role = "worker"
      }
    }
  }

  tags = {
    Environment = "dr"
    Purpose     = "disaster-recovery"
  }
}
```

### 6.3 Route 53 헬스체크 및 페일오버

```hcl
# terraform/dns/failover.tf

# Health check for primary region
resource "aws_route53_health_check" "primary_api" {
  fqdn              = "api-primary.kerp.io"
  port              = 443
  type              = "HTTPS"
  resource_path     = "/health"
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = "kerp-api-primary-health"
  }
}

# Health check for DR region
resource "aws_route53_health_check" "dr_api" {
  fqdn              = "api-dr.kerp.io"
  port              = 443
  type              = "HTTPS"
  resource_path     = "/health"
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = "kerp-api-dr-health"
  }
}

# Primary record with failover
resource "aws_route53_record" "api_primary" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "api.kerp.io"
  type    = "A"

  failover_routing_policy {
    type = "PRIMARY"
  }

  set_identifier  = "primary"
  health_check_id = aws_route53_health_check.primary_api.id

  alias {
    name                   = aws_lb.primary.dns_name
    zone_id                = aws_lb.primary.zone_id
    evaluate_target_health = true
  }
}

# DR record with failover
resource "aws_route53_record" "api_dr" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "api.kerp.io"
  type    = "A"

  failover_routing_policy {
    type = "SECONDARY"
  }

  set_identifier  = "dr"
  health_check_id = aws_route53_health_check.dr_api.id

  alias {
    name                   = aws_lb.dr.dns_name
    zone_id                = aws_lb.dr.zone_id
    evaluate_target_health = true
  }
}
```

---

## 7. 테스트 및 훈련

### 7.1 DR 테스트 유형

```yaml
dr_test_types:
  tabletop_exercise:
    frequency: "분기별"
    duration: "2-4시간"
    participants:
      - "DevOps 팀"
      - "개발 팀 리드"
      - "고객지원 팀"
    objectives:
      - "절차 검토"
      - "역할 확인"
      - "갭 식별"

  simulation_test:
    frequency: "반기별"
    duration: "4-8시간"
    scope: "일부 시스템"
    objectives:
      - "백업 복원 테스트"
      - "페일오버 절차 검증"
      - "통신 채널 테스트"

  full_dr_test:
    frequency: "연 1회"
    duration: "24시간"
    scope: "전체 시스템"
    objectives:
      - "실제 DR 환경 전환"
      - "비즈니스 연속성 검증"
      - "RTO/RPO 달성 확인"

  chaos_engineering:
    frequency: "월별"
    duration: "1-2시간"
    scope: "개별 컴포넌트"
    objectives:
      - "자동 복구 검증"
      - "장애 격리 테스트"
      - "시스템 복원력 향상"
```

### 7.2 테스트 자동화

```go
// internal/disaster/drtest/runner.go
package drtest

import (
    "context"
    "fmt"
    "time"
)

type DRTestRunner struct {
    config      *DRTestConfig
    metrics     *MetricsCollector
    notifier    Notifier
}

type DRTestConfig struct {
    TestType        string
    TargetRegion    string
    Scope           []string // Services to test
    DryRun          bool
    NotifyOnFailure bool
}

type TestResult struct {
    TestName    string
    Status      string
    Duration    time.Duration
    RTOAchieved time.Duration
    RPOAchieved time.Duration
    Details     map[string]interface{}
    Errors      []string
}

func (r *DRTestRunner) RunDRTest(ctx context.Context) (*TestResult, error) {
    result := &TestResult{
        TestName: fmt.Sprintf("DR-Test-%s", time.Now().Format("20060102-150405")),
    }

    startTime := time.Now()

    // Phase 1: Pre-flight checks
    if err := r.runPreflightChecks(ctx); err != nil {
        result.Status = "failed"
        result.Errors = append(result.Errors, err.Error())
        return result, err
    }

    // Phase 2: Simulate failure (if not dry run)
    if !r.config.DryRun {
        if err := r.simulateFailure(ctx); err != nil {
            result.Status = "failed"
            result.Errors = append(result.Errors, err.Error())
            return result, err
        }
    }

    // Phase 3: Measure failover time
    failoverStart := time.Now()
    if err := r.waitForFailover(ctx); err != nil {
        result.Status = "failed"
        result.Errors = append(result.Errors, err.Error())
        return result, err
    }
    result.RTOAchieved = time.Since(failoverStart)

    // Phase 4: Validate DR environment
    validationResult, err := r.validateDREnvironment(ctx)
    if err != nil {
        result.Status = "failed"
        result.Errors = append(result.Errors, err.Error())
    }
    result.Details = validationResult

    // Phase 5: Check data consistency
    rpoCheck, err := r.checkDataConsistency(ctx)
    if err != nil {
        result.Errors = append(result.Errors, err.Error())
    }
    result.RPOAchieved = rpoCheck

    // Phase 6: Failback (if not dry run)
    if !r.config.DryRun {
        if err := r.performFailback(ctx); err != nil {
            result.Errors = append(result.Errors, fmt.Sprintf("failback error: %v", err))
        }
    }

    result.Duration = time.Since(startTime)

    if len(result.Errors) == 0 {
        result.Status = "passed"
    } else {
        result.Status = "failed"
    }

    // Notify results
    r.notifier.NotifyTestResult(ctx, result)

    return result, nil
}

func (r *DRTestRunner) runPreflightChecks(ctx context.Context) error {
    checks := []func(context.Context) error{
        r.checkDRRegionAccessible,
        r.checkBackupsAvailable,
        r.checkReplicationStatus,
        r.checkDNSConfiguration,
    }

    for _, check := range checks {
        if err := check(ctx); err != nil {
            return err
        }
    }
    return nil
}

func (r *DRTestRunner) simulateFailure(ctx context.Context) error {
    // Simulate primary region failure
    // This could involve:
    // - Blocking network traffic
    // - Shutting down services
    // - Corrupting health checks
    return nil
}

func (r *DRTestRunner) waitForFailover(ctx context.Context) error {
    // Wait for automatic failover or trigger manual failover
    // Monitor until DR region becomes active
    return nil
}

func (r *DRTestRunner) validateDREnvironment(ctx context.Context) (map[string]interface{}, error) {
    result := make(map[string]interface{})

    // Check all services are running
    // Verify database connectivity
    // Test API endpoints
    // Validate external integrations

    return result, nil
}

func (r *DRTestRunner) checkDataConsistency(ctx context.Context) (time.Duration, error) {
    // Compare data between primary (before failure) and DR
    // Calculate actual RPO
    return time.Minute * 5, nil // Example: 5 minutes of data lag
}

func (r *DRTestRunner) performFailback(ctx context.Context) error {
    // Restore traffic to primary region
    // Resync data from DR to primary
    // Validate primary region
    return nil
}
```

### 7.3 테스트 체크리스트

```yaml
# docs/disaster/dr-test-checklist.yaml

dr_test_checklist:
  pre_test:
    - name: "백업 상태 확인"
      command: "aws s3 ls s3://kerp-backups-prod/postgresql/daily/ | tail -5"
      expected: "최근 5개 백업 파일 존재"

    - name: "복제 지연 확인"
      command: |
        aws rds describe-db-instances \
          --db-instance-identifier kerp-db-replica-dr \
          --query 'DBInstances[0].StatusInfos'
      expected: "ReplicaLag < 60 seconds"

    - name: "DR 클러스터 상태"
      command: "kubectl --context=dr get nodes"
      expected: "All nodes Ready (or scalable)"

  during_test:
    - name: "페일오버 시작"
      command: "./scripts/disaster/initiate_failover.sh"
      monitor: "CloudWatch 대시보드"

    - name: "DNS 전환 확인"
      command: "dig +short api.kerp.io"
      expected: "DR region ALB IP"

    - name: "서비스 가용성 확인"
      command: "curl -f https://api.kerp.io/health"
      expected: "HTTP 200"

  post_test:
    - name: "데이터 정합성 검증"
      command: "./scripts/disaster/validate_data.sh"
      expected: "모든 검증 통과"

    - name: "RTO 측정"
      expected: "< 4시간"

    - name: "RPO 측정"
      expected: "< 1시간"

    - name: "페일백 실행"
      command: "./scripts/disaster/failback.sh"
      expected: "Primary로 트래픽 복귀"
```

---

## 8. 커뮤니케이션 계획

### 8.1 에스컬레이션 매트릭스

```yaml
escalation_matrix:
  level1:
    name: "On-Call Engineer"
    response_time: "15분"
    authority:
      - "서비스 재시작"
      - "Pod 스케일링"
      - "알림 확인"
    escalate_to: "level2"
    escalation_trigger:
      - "30분 내 해결 불가"
      - "다중 서비스 영향"

  level2:
    name: "DevOps Lead"
    response_time: "30분"
    authority:
      - "인프라 변경"
      - "데이터베이스 failover"
      - "외부 업체 연락"
    escalate_to: "level3"
    escalation_trigger:
      - "1시간 내 해결 불가"
      - "데이터 손실 위험"

  level3:
    name: "CTO / Engineering Director"
    response_time: "1시간"
    authority:
      - "DR 활성화 결정"
      - "고객 커뮤니케이션 승인"
      - "비용 승인"
    escalate_to: "level4"
    escalation_trigger:
      - "주요 장애 지속"
      - "SLA 위반"

  level4:
    name: "Executive Team"
    response_time: "2시간"
    authority:
      - "언론 대응"
      - "법적 대응"
      - "보상 결정"
```

### 8.2 알림 템플릿

```go
// internal/disaster/notification.go
package disaster

import (
    "bytes"
    "html/template"
)

type IncidentNotification struct {
    IncidentID    string
    Severity      string
    Title         string
    Description   string
    StartTime     string
    Status        string
    AffectedArea  string
    Impact        string
    ActionsTaken  []string
    NextUpdate    string
}

const internalNotificationTemplate = `
[K-ERP 장애 알림]

장애 ID: {{.IncidentID}}
심각도: {{.Severity}}
제목: {{.Title}}

발생 시각: {{.StartTime}}
상태: {{.Status}}

영향 범위: {{.AffectedArea}}
영향: {{.Impact}}

조치 내역:
{{range .ActionsTaken}}
- {{.}}
{{end}}

다음 업데이트: {{.NextUpdate}}

문의: oncall@kerp.io
`

const customerNotificationTemplate = `
안녕하세요, K-ERP 고객님.

현재 일부 서비스에서 {{.Severity}} 수준의 장애가 발생하여 안내드립니다.

[장애 정보]
- 발생 시각: {{.StartTime}}
- 영향 서비스: {{.AffectedArea}}
- 현재 상태: {{.Status}}

{{.Description}}

복구를 위해 최선을 다하고 있으며, 진행 상황은 status.kerp.io에서 확인하실 수 있습니다.

불편을 드려 죄송합니다.

K-ERP 팀 드림
`

func (n *IncidentNotification) RenderInternal() (string, error) {
    tmpl, err := template.New("internal").Parse(internalNotificationTemplate)
    if err != nil {
        return "", err
    }

    var buf bytes.Buffer
    if err := tmpl.Execute(&buf, n); err != nil {
        return "", err
    }

    return buf.String(), nil
}

func (n *IncidentNotification) RenderCustomer() (string, error) {
    tmpl, err := template.New("customer").Parse(customerNotificationTemplate)
    if err != nil {
        return "", err
    }

    var buf bytes.Buffer
    if err := tmpl.Execute(&buf, n); err != nil {
        return "", err
    }

    return buf.String(), nil
}
```

### 8.3 상태 페이지

```typescript
// web/src/app/status/page.tsx
'use client';

import { useEffect, useState } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';

interface ServiceStatus {
  name: string;
  status: 'operational' | 'degraded' | 'outage' | 'maintenance';
  lastUpdated: string;
}

interface Incident {
  id: string;
  title: string;
  severity: 'minor' | 'major' | 'critical';
  status: 'investigating' | 'identified' | 'monitoring' | 'resolved';
  startedAt: string;
  updates: IncidentUpdate[];
}

interface IncidentUpdate {
  timestamp: string;
  status: string;
  message: string;
}

export default function StatusPage() {
  const [services, setServices] = useState<ServiceStatus[]>([]);
  const [incidents, setIncidents] = useState<Incident[]>([]);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchStatus();
    const interval = setInterval(fetchStatus, 30000);
    return () => clearInterval(interval);
  }, []);

  const fetchStatus = async () => {
    try {
      const [servicesRes, incidentsRes] = await Promise.all([
        fetch('/api/status/services'),
        fetch('/api/status/incidents'),
      ]);

      setServices(await servicesRes.json());
      setIncidents(await incidentsRes.json());
    } finally {
      setLoading(false);
    }
  };

  const getStatusColor = (status: ServiceStatus['status']) => {
    switch (status) {
      case 'operational':
        return 'bg-green-500';
      case 'degraded':
        return 'bg-yellow-500';
      case 'outage':
        return 'bg-red-500';
      case 'maintenance':
        return 'bg-blue-500';
    }
  };

  const getSeverityBadge = (severity: Incident['severity']) => {
    const colors = {
      minor: 'bg-yellow-100 text-yellow-800',
      major: 'bg-orange-100 text-orange-800',
      critical: 'bg-red-100 text-red-800',
    };
    return <Badge className={colors[severity]}>{severity.toUpperCase()}</Badge>;
  };

  if (loading) {
    return <div className="container mx-auto py-8">Loading...</div>;
  }

  const hasActiveIncident = incidents.some(
    (i) => i.status !== 'resolved'
  );

  return (
    <div className="container mx-auto py-8 space-y-8">
      <div className="text-center">
        <h1 className="text-3xl font-bold mb-4">K-ERP System Status</h1>
        <div
          className={`inline-flex items-center px-4 py-2 rounded-full ${
            hasActiveIncident
              ? 'bg-yellow-100 text-yellow-800'
              : 'bg-green-100 text-green-800'
          }`}
        >
          <span
            className={`w-3 h-3 rounded-full mr-2 ${
              hasActiveIncident ? 'bg-yellow-500' : 'bg-green-500'
            }`}
          />
          {hasActiveIncident
            ? 'Some services experiencing issues'
            : 'All Systems Operational'}
        </div>
      </div>

      <Card>
        <CardHeader>
          <CardTitle>Service Status</CardTitle>
        </CardHeader>
        <CardContent>
          <div className="space-y-4">
            {services.map((service) => (
              <div
                key={service.name}
                className="flex items-center justify-between py-2 border-b last:border-0"
              >
                <span className="font-medium">{service.name}</span>
                <div className="flex items-center gap-2">
                  <span
                    className={`w-3 h-3 rounded-full ${getStatusColor(
                      service.status
                    )}`}
                  />
                  <span className="text-sm capitalize">{service.status}</span>
                </div>
              </div>
            ))}
          </div>
        </CardContent>
      </Card>

      {incidents.length > 0 && (
        <Card>
          <CardHeader>
            <CardTitle>Active Incidents</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="space-y-6">
              {incidents
                .filter((i) => i.status !== 'resolved')
                .map((incident) => (
                  <div key={incident.id} className="border-l-4 border-red-500 pl-4">
                    <div className="flex items-center gap-2 mb-2">
                      {getSeverityBadge(incident.severity)}
                      <h3 className="font-semibold">{incident.title}</h3>
                    </div>
                    <div className="space-y-2">
                      {incident.updates.map((update, idx) => (
                        <div key={idx} className="text-sm">
                          <span className="text-gray-500">
                            {new Date(update.timestamp).toLocaleString()}
                          </span>
                          <span className="mx-2">-</span>
                          <span className="font-medium">{update.status}:</span>
                          <span className="ml-1">{update.message}</span>
                        </div>
                      ))}
                    </div>
                  </div>
                ))}
            </div>
          </CardContent>
        </Card>
      )}

      <div className="text-center text-sm text-gray-500">
        Last updated: {new Date().toLocaleString()}
      </div>
    </div>
  );
}
```

---

## 버전 이력

| 버전 | 날짜 | 작성자 | 변경 내용 |
|------|------|--------|-----------|
| 0.2 | 2024-01-15 | DevOps Team | Go+Python 하이브리드 아키텍처 반영 |
| 0.1 | 2024-01-01 | DevOps Team | 초안 작성 |
